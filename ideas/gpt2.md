# Idea

A gpt2 themed transformer and a tokenizer

## Why

   - First thing first the idea is real cool as a learning project to get started with LLMs in machine learning
   - Teaches basic yet very powerful concepts like attention mechanism and Transformers.
   - Is a good project to put on a cv

## How 
  
  - There are already methods and videos to implement standard gpt 2 model but that's lame
  - Exploring what new open source LLMs like Gemma and Ollama uses and incorporating all that to get an even better model.

## Challenges Faced

 - Biggest Challenge will be collecting data and data cleaning.
 - Trying not to copy code from internet without any novelty.

## Good to have features

 - Having RAG layers

contributed by BufferFis https://github.com/bufferfis
